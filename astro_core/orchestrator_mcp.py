#!/usr/bin/env python3
"""
MCP Workflow Orchestrator Server - AstroGenAI
Serveur MCP pour orchestration intelligente des workflows d'horoscopes
Agent coordinateur utilisant Ollama pour la planification et l'optimisation
"""

import os
import re
import sys
import datetime
import json
import asyncio
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import time
import logging

try:
    from fastmcp import FastMCP
    FASTMCP_AVAILABLE = True
except ImportError:
    FASTMCP_AVAILABLE = False
    print("‚ö†Ô∏è FastMCP non disponible")

try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    print("‚ö†Ô∏è Ollama non disponible")


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# =============================================================================
# CONFIGURATION ET DATACLASSES
# =============================================================================

@dataclass
class WorkflowStep:
    """√âtape d'un workflow"""
    service: str
    action: str
    params: Dict[str, Any]
    priority: int = 1
    retry_count: int = 0
    max_retries: int = 3
    estimated_duration: float = 0.0
    dependencies: List[str] = None
    
    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = []

@dataclass
class WorkflowPlan:
    """Plan d'ex√©cution d'un workflow"""
    workflow_id: str
    steps: List[WorkflowStep]
    execution_strategy: str  # "sequential", "parallel", "hybrid"
    estimated_total_duration: float
    resource_requirements: Dict[str, Any]
    fallback_strategy: str
    optimization_goals: List[str]
    created_at: str

@dataclass
class ExecutionResult:
    """R√©sultat d'ex√©cution d'un workflow"""
    workflow_id: str
    success: bool
    completed_steps: List[str]
    failed_steps: List[str]
    execution_time: float
    results: Dict[str, Any]
    optimizations_applied: List[str]
    performance_metrics: Dict[str, float]
    agent_insights: Dict[str, str]

@dataclass
class ServiceStatus:
    """√âtat d'un service MCP"""
    name: str
    available: bool
    load: float  # 0.0 √† 1.0
    last_response_time: float
    error_rate: float
    capabilities: List[str]

# =============================================================================
# ORCHESTRATEUR INTELLIGENT
# =============================================================================

class WorkflowOrchestrator:
    """Orchestrateur de workflows intelligent utilisant Ollama"""
    
    def __init__(self, ollama_model: str = "llama3.1:8b-instruct-q8_0"):
        self.ollama_model = ollama_model
        self.active_workflows = {}
        self.service_registry = {}
        self.performance_history = []
        
        # Configuration de l'agent
        self.agent_contexts = {
            "planning": {
                "temperature": 0.2,
                "system_prompt": self._get_planning_prompt(),
                "max_tokens": 800
            },
            "optimization": {
                "temperature": 0.3,
                "system_prompt": self._get_optimization_prompt(),
                "max_tokens": 600
            },
            "recovery": {
                "temperature": 0.4,
                "system_prompt": self._get_recovery_prompt(),
                "max_tokens": 500
            },
            "analysis": {
                "temperature": 0.1,
                "system_prompt": self._get_analysis_prompt(),
                "max_tokens": 700
            }
        }
    
    def _get_planning_prompt(self) -> str:
        return """Tu es un orchestrateur de workflows d'horoscopes IA expert.

MISSION: Cr√©er des plans d'ex√©cution optimaux pour les demandes utilisateur.

SERVICES DISPONIBLES:
- astro_server: G√©n√©ration horoscopes + audio (TTS)
- astrochart_server : Generation des positions plan√©taires et calculs astronomiques
- video_server: Montage vid√©o avec synchronisation
- comfyui_server: G√©n√©ration vid√©os constellations
- youtube_server: Upload automatique sur YouTube

STRAT√âGIES D'EX√âCUTION:
- sequential: Une √©tape apr√®s l'autre (s√ªr, lent)
- parallel: Plusieurs √©tapes simultan√©es (rapide, consomme plus)
- hybrid: Mix intelligent selon les d√©pendances

OPTIMISATIONS POSSIBLES:
- R√©utilisation de contenu existant
- Parall√©lisation intelligente
- Pr√©diction de charge
- Gestion proactive d'erreurs

R√©ponds TOUJOURS en JSON valide avec un plan d√©taill√©."""

    def _get_optimization_prompt(self) -> str:
        return """Tu es un optimiseur de performance pour workflows d'horoscopes IA.

MISSION: Analyser et optimiser les workflows en cours d'ex√©cution.

M√âTRIQUES IMPORTANTES:
- Temps d'ex√©cution total
- Utilisation des ressources
- Taux d'erreur
- Qualit√© du contenu g√©n√©r√©
- Engagement utilisateur

OPTIMISATIONS DISPONIBLES:
- Cache intelligent des horoscopes
- Pr√©-g√©n√©ration de contenu populaire
- Load balancing des services
- Compression/qualit√© des vid√©os
- Timing optimal pour upload

Sugg√®re des am√©liorations concr√®tes en JSON."""

    def _get_recovery_prompt(self) -> str:
        return """Tu es un expert en r√©cup√©ration d'erreurs pour workflows IA.

MISSION: Analyser les √©checs et proposer des solutions de r√©cup√©ration.

STRAT√âGIES DE R√âCUP√âRATION:
- Retry avec param√®tres modifi√©s
- Contournement par service alternatif
- D√©gradation gracieuse de qualit√©
- Workflow partiel avec notification
- Reprogrammation diff√©r√©e

ANALYSE REQUISE:
- Type d'erreur et cause probable
- Impact sur le workflow global
- Services affect√©s vs disponibles
- Faisabilit√© des alternatives

Propose un plan de r√©cup√©ration en JSON."""

    def _get_analysis_prompt(self) -> str:
        return """Tu es un analyste de performance pour workflows d'horoscopes IA.

MISSION: Analyser les r√©sultats et extraire des insights.

ANALYSES √Ä EFFECTUER:
- Performance comparative des services
- Tendances d'utilisation
- Goulots d'√©tranglement identifi√©s
- Pr√©dictions de charge future
- Recommandations d'am√©lioration

M√âTRIQUES CL√âS:
- Temps de r√©ponse par service
- Taux de succ√®s des workflows
- Satisfaction utilisateur estim√©e
- Efficacit√© des ressources

Fournis des insights actionnables en JSON."""

    async def _call_ollama_with_context(self, context_type: str, prompt: str) -> Dict[str, Any]:
        """Appelle Ollama avec un contexte sp√©cifique"""
        if not OLLAMA_AVAILABLE:
            raise Exception("Ollama non disponible")
        
        context = self.agent_contexts.get(context_type, self.agent_contexts["planning"])
        
        full_prompt = f"{context['system_prompt']}\n\n{prompt}\n\nR√©ponds UNIQUEMENT avec un JSON valide, sans texte additionnel."
        
        try:
            response = ollama.chat(
                model=self.ollama_model,
                messages=[{'role': 'user', 'content': full_prompt}],
                options={
                    'temperature': context['temperature'],
                    'top_p': 0.9,
                    'num_predict': context['max_tokens']
                }
            )
            
            content = response['message']['content'].strip()
            
            # Debug: Afficher la r√©ponse brute
            print(f"üîç R√©ponse Ollama brute ({context_type}):")
            print(content[:500] + "..." if len(content) > 500 else content)
            print("=" * 50)
            
            # M√©thode 1: JSON direct
            try:
                return json.loads(content)
            except json.JSONDecodeError:
                pass
            
            # M√©thode 2: Extraire le JSON entre { }
            import re
            json_patterns = [
                r'\{.*?\}',  # JSON simple
                r'\{[\s\S]*\}',  # JSON avec newlines
            ]
            
            for pattern in json_patterns:
                json_match = re.search(pattern, content, re.DOTALL)
                if json_match:
                    try:
                        json_text = json_match.group()
                        # Nettoyer le JSON
                        json_text = self._clean_json_text(json_text)
                        return json.loads(json_text)
                    except json.JSONDecodeError:
                        continue
            
            # M√©thode 3: Fallback avec structure par d√©faut
            print("‚ö†Ô∏è  JSON parsing √©chou√©, utilisation de fallback intelligent")
            return self._create_fallback_response(context_type, content, prompt)
                    
        except Exception as e:
            raise Exception(f"Erreur Ollama: {e}")

    def _clean_json_text(self, json_text: str) -> str:
        """Nettoie le texte JSON pour le parsing"""
        # Supprimer les commentaires
        json_text = re.sub(r'//.*?\n', '\n', json_text)
        json_text = re.sub(r'/\*.*?\*/', '', json_text, flags=re.DOTALL)
        
        # Corriger les quotes simples en doubles
        json_text = re.sub(r"'([^']*)':", r'"\1":', json_text)
        json_text = re.sub(r': *\'([^\']*)\'', r': "\1"', json_text)
        
        # Supprimer les virgules en fin de ligne
        json_text = re.sub(r',(\s*[}\]])', r'\1', json_text)
        
        # Supprimer les caract√®res de contr√¥le
        json_text = ''.join(char for char in json_text if ord(char) >= 32 or char in '\n\r\t')
        
        return json_text.strip()

    def _create_fallback_response(self, context_type: str, raw_content: str, original_prompt: str) -> Dict[str, Any]:
        """Cr√©e une r√©ponse de fallback intelligente"""
        
        if context_type == "planning":
            # Parser intelligent pour extraction d'informations
            signs_mentioned = []
            for sign in ["aries", "taurus", "gemini", "cancer", "leo", "virgo", 
                        "libra", "scorpio", "sagittarius", "capricorn", "aquarius", "pisces"]:
                if sign in original_prompt.lower():
                    signs_mentioned.append(sign)
            
            # D√©terminer la strat√©gie bas√©e sur le contenu
            if "parallel" in raw_content.lower() or "simultan√©" in raw_content.lower():
                strategy = "parallel"
            elif "hybrid" in raw_content.lower() or "hybride" in raw_content.lower():
                strategy = "hybrid"
            else:
                strategy = "sequential"
            
            # Cr√©er des √©tapes intelligentes
            steps = []
            base_services = ["astrochart_server", "astro_server", "video_server"]
            
            if "comfyui" in original_prompt.lower() or "video" in original_prompt.lower():
                base_services.append("comfyui_server")
            
            if "youtube" in original_prompt.lower() or "upload" in original_prompt.lower():
                base_services.append("youtube_server")
            
            for i, service in enumerate(base_services):
                steps.append({
                    "service": service,
                    "action": "generate" if "generate" in service else "process",
                    "params": {"signs": signs_mentioned} if signs_mentioned else {},
                    "priority": i + 1,
                    "estimated_duration": 30.0 * (i + 1),
                    "dependencies": [base_services[i-1]] if i > 0 else []
                })
            
            return {
                "execution_strategy": strategy,
                "steps": steps,
                "estimated_total_duration": len(steps) * 45.0,
                "resource_requirements": {
                    "cpu_intensive": True,
                    "io_intensive": True,
                    "memory_usage": "medium"
                },
                "fallback_strategy": "Retry avec param√®tres simplifi√©s",
                "optimization_goals": ["quality", "speed"],
                "reasoning": f"Plan de fallback bas√© sur analyse du prompt. Strat√©gie: {strategy}",
                "fallback_used": True
            }
        
        elif context_type == "optimization":
            return {
                "performance_assessment": "good",
                "bottlenecks_identified": ["parsing_issues"],
                "optimization_recommendations": ["am√©liorer_prompts", "fallback_robuste"],
                "future_predictions": "Performance stable avec fallbacks",
                "learning_points": ["JSON parsing n√©cessite robustesse"],
                "fallback_used": True
            }
        
        elif context_type == "recovery":
            return {
                "recovery_possible": True,
                "recovery_strategy": "Utiliser workflow simplifi√© avec fallbacks",
                "modified_steps": [
                    {"service": "astro_server", "action": "generate_simple", "params": {}}
                ],
                "estimated_success_probability": 0.8,
                "reasoning": "Fallback vers approche simplifi√©e",
                "fallback_used": True
            }
        
        else:  # analysis
            return {
                "performance_trends": "Stable avec adaptation n√©cessaire",
                "bottlenecks_identified": ["JSON parsing", "prompt complexity"],
                "success_factors": ["fallback mechanisms", "robust error handling"],
                "optimization_opportunities": ["prompt engineering", "response parsing"],
                "strategic_recommendations": ["Impl√©menter fallbacks syst√©matiques"],
                "predicted_improvements": "Am√©lioration de robustesse avec fallbacks",
                "fallback_used": True
            }

    # =============================================================================
    # M√âTHODES DE PLANIFICATION
    # =============================================================================

    async def create_intelligent_plan(self, user_request: Dict[str, Any]) -> WorkflowPlan:
        """Cr√©e un plan de workflow intelligent"""
        
        # Analyser la demande avec l'agent
        planning_prompt = f"""
        DEMANDE UTILISATEUR:
        {json.dumps(user_request, indent=2)}
        
        SERVICES ACTUELLEMENT DISPONIBLES:
        {json.dumps(self._get_services_status(), indent=2)}
        
        HISTORIQUE DE PERFORMANCE:
        {json.dumps(self._get_performance_summary(), indent=2)}
        
        Cr√©e un plan d'ex√©cution optimal. R√©ponds en JSON avec cette structure:
        {{
            "execution_strategy": "sequential|parallel|hybrid",
            "steps": [
                {{
                    "service": "nom_service",
                    "action": "action_√†_effectuer", 
                    "params": {{}},
                    "priority": 1-5,
                    "estimated_duration": 0.0,
                    "dependencies": []
                }}
            ],
            "estimated_total_duration": 0.0,
            "resource_requirements": {{
                "cpu_intensive": true/false,
                "io_intensive": true/false,
                "memory_usage": "low|medium|high"
            }},
            "fallback_strategy": "description",
            "optimization_goals": ["speed", "quality", "resource_efficiency"],
            "reasoning": "explication du plan choisi"
        }}
        """
        
        agent_response = await self._call_ollama_with_context("planning", planning_prompt)
        
        # Construire le plan
        workflow_id = f"workflow_{int(time.time())}"
        
        steps = []
        for step_data in agent_response.get("steps", []):
            step = WorkflowStep(
                service=step_data["service"],
                action=step_data["action"],
                params=step_data["params"],
                priority=step_data.get("priority", 1),
                estimated_duration=step_data.get("estimated_duration", 30.0),
                dependencies=step_data.get("dependencies", [])
            )
            steps.append(step)
        
        plan = WorkflowPlan(
            workflow_id=workflow_id,
            steps=steps,
            execution_strategy=agent_response.get("execution_strategy", "sequential"),
            estimated_total_duration=agent_response.get("estimated_total_duration", 120.0),
            resource_requirements=agent_response.get("resource_requirements", {}),
            fallback_strategy=agent_response.get("fallback_strategy", "Retry avec param√®tres r√©duits"),
            optimization_goals=agent_response.get("optimization_goals", ["quality"]),
            created_at=datetime.datetime.now().isoformat()
        )
        
        self.active_workflows[workflow_id] = plan
        return plan

    async def execute_workflow_plan(self, plan: WorkflowPlan) -> ExecutionResult:
        """Ex√©cute un plan de workflow avec monitoring intelligent"""
        
        start_time = time.time()
        completed_steps = []
        failed_steps = []
        results = {}
        optimizations_applied = []
        
        try:
            if plan.execution_strategy == "sequential":
                results = await self._execute_sequential(plan, completed_steps, failed_steps)
            elif plan.execution_strategy == "parallel":
                results = await self._execute_parallel(plan, completed_steps, failed_steps)
            else:  # hybrid
                results = await self._execute_hybrid(plan, completed_steps, failed_steps)
                
        except Exception as e:
            # Tentative de r√©cup√©ration intelligente
            recovery_result = await self._attempt_intelligent_recovery(plan, str(e), completed_steps)
            if recovery_result["success"]:
                optimizations_applied.append("intelligent_error_recovery")
                results.update(recovery_result["results"])
            else:
                failed_steps.append("workflow_execution")
        
        execution_time = time.time() - start_time
        
        # Analyse post-ex√©cution
        performance_metrics = await self._analyze_performance(plan, execution_time, completed_steps, failed_steps)
        agent_insights = await self._generate_insights(plan, results, performance_metrics)
        
        result = ExecutionResult(
            workflow_id=plan.workflow_id,
            success=len(failed_steps) == 0,
            completed_steps=completed_steps,
            failed_steps=failed_steps,
            execution_time=execution_time,
            results=results,
            optimizations_applied=optimizations_applied,
            performance_metrics=performance_metrics,
            agent_insights=agent_insights
        )
        
        # Sauvegarder pour apprentissage
        self.performance_history.append(asdict(result))
        
        return result

    # =============================================================================
    # M√âTHODES D'EX√âCUTION
    # =============================================================================

    async def _execute_sequential(self, plan: WorkflowPlan, completed: List, failed: List) -> Dict:
        """Ex√©cution s√©quentielle avec optimisations"""
        results = {}
        
        for step in plan.steps:
            try:
                step_result = await self._execute_step(step)
                results[step.service] = step_result
                completed.append(f"{step.service}:{step.action}")
                
                # Optimization: early exit si suffisant
                if await self._should_early_exit(step_result, plan):
                    break
                    
            except Exception as e:
                failed.append(f"{step.service}:{step.action}")
                if not await self._can_continue_after_failure(step, plan):
                    break
        
        return results

    async def _execute_parallel(self, plan: WorkflowPlan, completed: List, failed: List) -> Dict:
        """Ex√©cution parall√®le avec gestion des d√©pendances"""
        results = {}
        
        # Grouper par niveau de d√©pendances
        execution_groups = self._group_by_dependencies(plan.steps)
        
        for group in execution_groups:
            # Ex√©cuter le groupe en parall√®le
            tasks = [self._execute_step(step) for step in group]
            group_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(group_results):
                step = group[i]
                if isinstance(result, Exception):
                    failed.append(f"{step.service}:{step.action}")
                else:
                    results[step.service] = result
                    completed.append(f"{step.service}:{step.action}")
        
        return results

    async def _execute_hybrid(self, plan: WorkflowPlan, completed: List, failed: List) -> Dict:
        """Ex√©cution hybride - le plus intelligent"""
        # L'agent d√©cide dynamiquement de la strat√©gie pour chaque √©tape
        optimization_prompt = f"""
        WORKFLOW EN COURS:
        Plan: {plan.execution_strategy}
        √âtapes restantes: {len([s for s in plan.steps if f"{s.service}:{s.action}" not in completed])}
        √âchecs: {failed}
        
        Quelle est la meilleure strat√©gie pour les √©tapes suivantes?
        
        R√©ponds en JSON:
        {{
            "next_strategy": "sequential|parallel",
            "reasoning": "pourquoi cette strat√©gie",
            "optimizations": ["liste des optimisations √† appliquer"]
        }}
        """
        
        strategy_response = await self._call_ollama_with_context("optimization", optimization_prompt)
        
        # Ex√©cuter selon la strat√©gie dynamique
        if strategy_response.get("next_strategy") == "parallel":
            return await self._execute_parallel(plan, completed, failed)
        else:
            return await self._execute_sequential(plan, completed, failed)

    async def _execute_step(self, step: WorkflowStep) -> Dict[str, Any]:
        """Ex√©cute une √©tape individuelle"""
        # Ici, vous feriez des appels aux vrais services MCP
        # Pour l'exemple, simulation
        
        await asyncio.sleep(step.estimated_duration / 10)  # Simulation
        
        return {
            "service": step.service,
            "action": step.action,
            "success": True,
            "result": f"R√©sultat simul√© pour {step.service}:{step.action}",
            "execution_time": step.estimated_duration / 10
        }

    # =============================================================================
    # M√âTHODES D'ANALYSE ET OPTIMISATION
    # =============================================================================

    async def _analyze_performance(self, plan: WorkflowPlan, execution_time: float, 
                                 completed: List, failed: List) -> Dict[str, float]:
        """Analyse les performances du workflow"""
        return {
            "total_execution_time": execution_time,
            "planned_vs_actual_ratio": execution_time / plan.estimated_total_duration,
            "success_rate": len(completed) / (len(completed) + len(failed)) if (completed or failed) else 1.0,
            "efficiency_score": len(completed) / execution_time if execution_time > 0 else 0.0,
            "resource_utilization": 0.75  # Simulation
        }

    async def _generate_insights(self, plan: WorkflowPlan, results: Dict, 
                               metrics: Dict[str, float]) -> Dict[str, str]:
        """G√©n√®re des insights intelligents"""
        
        analysis_prompt = f"""
        WORKFLOW TERMIN√â:
        Plan original: {plan.execution_strategy}
        R√©sultats: {len(results)} services ex√©cut√©s
        M√©triques: {json.dumps(metrics, indent=2)}
        
        G√©n√®re des insights actionnables. R√©ponds en JSON:
        {{
            "performance_assessment": "excellent|good|fair|poor",
            "bottlenecks_identified": ["liste des goulots"],
            "optimization_recommendations": ["recommandations concr√®tes"],
            "future_predictions": "pr√©dictions pour workflows similaires",
            "learning_points": ["points d'apprentissage pour l'IA"]
        }}
        """
        
        return await self._call_ollama_with_context("analysis", analysis_prompt)

    # =============================================================================
    # M√âTHODES UTILITAIRES
    # =============================================================================

    def _get_services_status(self) -> Dict[str, Any]:
        """Obtient l'√©tat des services (simulation)"""
        return {
            "astrochart_server": {"available": True, "load": 0.3},
            "astro_server": {"available": True, "load": 0.3},
            "video_server": {"available": True, "load": 0.7},
            "comfyui_server": {"available": True, "load": 0.5},
            "youtube_server": {"available": True, "load": 0.2}
        }

    def _get_performance_summary(self) -> Dict[str, Any]:
        """R√©sum√© des performances historiques"""
        if not self.performance_history:
            return {"average_execution_time": 120.0, "success_rate": 0.95}
        
        recent = self.performance_history[-10:]  # 10 derniers
        avg_time = sum(r["execution_time"] for r in recent) / len(recent)
        success_rate = sum(1 for r in recent if r["success"]) / len(recent)
        
        return {
            "average_execution_time": avg_time,
            "success_rate": success_rate,
            "total_workflows": len(self.performance_history)
        }

    def _group_by_dependencies(self, steps: List[WorkflowStep]) -> List[List[WorkflowStep]]:
        """Groupe les √©tapes par niveau de d√©pendances"""
        groups = []
        remaining = steps.copy()
        
        while remaining:
            current_group = []
            for step in remaining[:]:
                if not step.dependencies or all(dep in [s.service for group in groups for s in group] for dep in step.dependencies):
                    current_group.append(step)
                    remaining.remove(step)
            
            if current_group:
                groups.append(current_group)
            else:
                # √âviter les boucles infinies
                groups.append(remaining)
                break
        
        return groups

    async def _attempt_intelligent_recovery(self, plan: WorkflowPlan, error: str, 
                                          completed: List) -> Dict[str, Any]:
        """Tentative de r√©cup√©ration intelligente"""
        
        recovery_prompt = f"""
        √âCHEC DE WORKFLOW:
        Erreur: {error}
        √âtapes compl√©t√©es: {completed}
        Plan original: {plan.execution_strategy}
        
        Propose une strat√©gie de r√©cup√©ration. R√©ponds en JSON:
        {{
            "recovery_possible": true/false,
            "recovery_strategy": "description",
            "modified_steps": [
                {{"service": "...", "action": "...", "params": {{}}}}
            ],
            "estimated_success_probability": 0.0-1.0,
            "reasoning": "explication"
        }}
        """
        
        recovery_plan = await self._call_ollama_with_context("recovery", recovery_prompt)
        
        if recovery_plan.get("recovery_possible", False):
            # Tenter l'ex√©cution du plan de r√©cup√©ration
            try:
                recovery_results = {}
                for step_data in recovery_plan.get("modified_steps", []):
                    # Simulation d'ex√©cution de r√©cup√©ration
                    recovery_results[step_data["service"]] = {
                        "success": True,
                        "recovered": True,
                        "result": f"R√©cup√©ration r√©ussie pour {step_data['service']}"
                    }
                
                return {"success": True, "results": recovery_results}
            except:
                return {"success": False, "results": {}}
        
        return {"success": False, "results": {}}

    async def _should_early_exit(self, step_result: Dict, plan: WorkflowPlan) -> bool:
        """D√©termine si on peut arr√™ter le workflow plus t√¥t"""
        # Logic d'optimisation: si le r√©sultat est suffisant
        return False  # Simulation

    async def _can_continue_after_failure(self, failed_step: WorkflowStep, plan: WorkflowPlan) -> bool:
        """D√©termine si on peut continuer apr√®s un √©chec"""
        # Logic: continuer si l'√©tape n'est pas critique
        return failed_step.priority <= 3  # Les priorit√©s 4-5 sont critiques

    # =============================================================================
    # M√âTHODES D'INTERFACE (pour tests et int√©gration)
    # =============================================================================

    def get_orchestrator_status(self) -> Dict[str, Any]:
        """Retourne l'√©tat de l'orchestrateur (m√©thode de classe)"""
        try:
            return {
                "success": True,
                "status": {
                    "ollama_available": OLLAMA_AVAILABLE,
                    "fastmcp_available": FASTMCP_AVAILABLE,
                    "active_workflows": len(self.active_workflows),
                    "total_workflows_processed": len(self.performance_history),
                    "average_success_rate": self._get_performance_summary().get("success_rate", 0.0),
                    "agent_contexts": list(self.agent_contexts.keys()),
                    "capabilities": [
                        "intelligent_planning",
                        "adaptive_execution", 
                        "error_recovery",
                        "performance_optimization",
                        "multi_strategy_execution"
                    ]
                }
            }
        except Exception as e:
            return {"success": False, "error": str(e)}

    async def analyze_workflow_performance_method(self, time_range_days: int = 7) -> Dict[str, Any]:
        """Analyse les performances (m√©thode de classe pour int√©gration)"""
        try:
            # Filtrer l'historique par p√©riode
            cutoff_date = datetime.datetime.now() - datetime.timedelta(days=time_range_days)
            recent_workflows = [
                w for w in self.performance_history 
                if datetime.datetime.fromisoformat(w.get("created_at", "1970-01-01T00:00:00")) > cutoff_date
            ]
            
            if not recent_workflows:
                return {
                    "success": True,
                    "analysis": {"message": "Aucun workflow dans la p√©riode sp√©cifi√©e"},
                    "recommendations": []
                }
            
            # Analyse avec l'agent
            analysis_prompt = f"""
            DONN√âES DE PERFORMANCE ({time_range_days} derniers jours):
            Workflows analys√©s: {len(recent_workflows)}
            
            M√âTRIQUES GLOBALES:
            {json.dumps({
                "total_workflows": len(recent_workflows),
                "avg_execution_time": sum(w["execution_time"] for w in recent_workflows) / len(recent_workflows),
                "success_rate": sum(1 for w in recent_workflows if w["success"]) / len(recent_workflows),
                "most_used_strategies": {}  # √Ä calculer
            }, indent=2)}
            
            Fournis une analyse compl√®te en JSON:
            {{
                "performance_trends": "analyse des tendances",
                "bottlenecks_identified": ["liste des probl√®mes"],
                "success_factors": ["facteurs de r√©ussite"],
                "optimization_opportunities": ["opportunit√©s d'am√©lioration"],
                "strategic_recommendations": ["recommandations strat√©giques"],
                "predicted_improvements": "pr√©dictions si recommandations appliqu√©es"
            }}
            """
            
            analysis = await self._call_ollama_with_context("analysis", analysis_prompt)
            
            return {
                "success": True,
                "period_analyzed": f"{time_range_days} jours",
                "workflows_count": len(recent_workflows),
                "analysis": analysis,
                "raw_metrics": {
                    "total_execution_time": sum(w["execution_time"] for w in recent_workflows),
                    "average_execution_time": sum(w["execution_time"] for w in recent_workflows) / len(recent_workflows),
                    "success_rate": sum(1 for w in recent_workflows if w["success"]) / len(recent_workflows)
                }
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}

# =============================================================================
# INITIALISATION ET SERVEUR MCP
# =============================================================================

# Instance globale de l'orchestrateur
workflow_orchestrator = WorkflowOrchestrator()

if FASTMCP_AVAILABLE:
    mcp = FastMCP("WorkflowOrchestrator")

    @mcp.tool()
    async def create_intelligent_workflow(user_request: Dict[str, Any]) -> dict:
        """
        Cr√©e et ex√©cute un workflow intelligent bas√© sur la demande utilisateur.
        
        Args:
            user_request: Demande utilisateur avec param√®tres et pr√©f√©rences
        
        Returns:
            Plan et r√©sultats d'ex√©cution du workflow
        """
        try:
            # L'agent analyse et planifie
            plan = await workflow_orchestrator.create_intelligent_plan(user_request)
            
            # Ex√©cution du plan
            result = await workflow_orchestrator.execute_workflow_plan(plan)
            
            return {
                "success": True,
                "workflow_plan": asdict(plan),
                "execution_result": asdict(result),
                "agent_recommendations": result.agent_insights
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}

    @mcp.tool()
    async def optimize_existing_workflow(workflow_id: str, optimization_goals: List[str]) -> dict:
        """
        Optimise un workflow existant selon des objectifs sp√©cifiques.
        
        Args:
            workflow_id: ID du workflow √† optimiser
            optimization_goals: Objectifs d'optimisation (speed, quality, cost, etc.)
        
        Returns:
            Plan optimis√© et recommandations
        """
        try:
            if workflow_id not in workflow_orchestrator.active_workflows:
                return {"success": False, "error": "Workflow non trouv√©"}
            
            current_plan = workflow_orchestrator.active_workflows[workflow_id]
            
            optimization_prompt = f"""
            WORKFLOW √Ä OPTIMISER:
            {json.dumps(asdict(current_plan), indent=2)}
            
            OBJECTIFS D'OPTIMISATION:
            {optimization_goals}
            
            Propose des optimisations. R√©ponds en JSON:
            {{
                "optimized_strategy": "nouvelle strat√©gie",
                "modified_steps": [{{...}}],
                "expected_improvements": {{
                    "time_reduction": "pourcentage",
                    "quality_improvement": "description",
                    "resource_savings": "description"
                }},
                "implementation_priority": "high|medium|low",
                "recommendations": ["liste de recommandations"]
            }}
            """
            
            optimizations = await workflow_orchestrator._call_ollama_with_context(
                "optimization", optimization_prompt
            )
            
            return {
                "success": True,
                "current_plan": asdict(current_plan),
                "optimizations": optimizations,
                "implementation_ready": True
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}

    @mcp.tool()
    def get_orchestrator_status() -> dict:
        """
        Retourne l'√©tat de l'orchestrateur et des workflows actifs.
        
        Returns:
            √âtat d√©taill√© de l'orchestrateur
        """
        try:
            return {
                "success": True,
                "status": {
                    "ollama_available": OLLAMA_AVAILABLE,
                    "fastmcp_available": FASTMCP_AVAILABLE,
                    "active_workflows": len(workflow_orchestrator.active_workflows),
                    "total_workflows_processed": len(workflow_orchestrator.performance_history),
                    "average_success_rate": workflow_orchestrator._get_performance_summary().get("success_rate", 0.0),
                    "agent_contexts": list(workflow_orchestrator.agent_contexts.keys()),
                    "capabilities": [
                        "intelligent_planning",
                        "adaptive_execution", 
                        "error_recovery",
                        "performance_optimization",
                        "multi_strategy_execution"
                    ]
                }
            }
        except Exception as e:
            return {"success": False, "error": str(e)}

    @mcp.tool() 
    async def analyze_workflow_performance(time_range_days: int = 7) -> dict:
        """
        Analyse les performances des workflows sur une p√©riode donn√©e.
        
        Args:
            time_range_days: P√©riode d'analyse en jours
            
        Returns:
            Analyse d√©taill√©e des performances
        """
        try:
            # Filtrer l'historique par p√©riode
            cutoff_date = datetime.datetime.now() - datetime.timedelta(days=time_range_days)
            recent_workflows = [
                w for w in workflow_orchestrator.performance_history 
                if datetime.datetime.fromisoformat(w.get("created_at", "1970-01-01")) > cutoff_date
            ]
            
            if not recent_workflows:
                return {
                    "success": True,
                    "analysis": {"message": "Aucun workflow dans la p√©riode sp√©cifi√©e"},
                    "recommendations": []
                }
            
            # Analyse avec l'agent
            analysis_prompt = f"""
            DONN√âES DE PERFORMANCE ({time_range_days} derniers jours):
            Workflows analys√©s: {len(recent_workflows)}
            
            M√âTRIQUES GLOBALES:
            {json.dumps({
                "total_workflows": len(recent_workflows),
                "avg_execution_time": sum(w["execution_time"] for w in recent_workflows) / len(recent_workflows),
                "success_rate": sum(1 for w in recent_workflows if w["success"]) / len(recent_workflows),
                "most_used_strategies": {}  # √Ä calculer
            }, indent=2)}
            
            Fournis une analyse compl√®te en JSON:
            {{
                "performance_trends": "analyse des tendances",
                "bottlenecks_identified": ["liste des probl√®mes"],
                "success_factors": ["facteurs de r√©ussite"],
                "optimization_opportunities": ["opportunit√©s d'am√©lioration"],
                "strategic_recommendations": ["recommandations strat√©giques"],
                "predicted_improvements": "pr√©dictions si recommandations appliqu√©es"
            }}
            """
            
            analysis = await workflow_orchestrator._call_ollama_with_context(
                "analysis", analysis_prompt
            )
            
            return {
                "success": True,
                "period_analyzed": f"{time_range_days} jours",
                "workflows_count": len(recent_workflows),
                "analysis": analysis,
                "raw_metrics": {
                    "total_execution_time": sum(w["execution_time"] for w in recent_workflows),
                    "average_execution_time": sum(w["execution_time"] for w in recent_workflows) / len(recent_workflows),
                    "success_rate": sum(1 for w in recent_workflows if w["success"]) / len(recent_workflows)
                }
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}

if __name__ == "__main__":
    print("ü§ñ" + "="*60)
    print("ü§ñ WORKFLOW ORCHESTRATOR MCP - AGENT INTELLIGENT")
    print("ü§ñ" + "="*60)
    
    # V√©rification des d√©pendances
    print(f"üì¶ D√©pendances:")
    print(f"   ‚Ä¢ Ollama: {'‚úÖ' if OLLAMA_AVAILABLE else '‚ùå'}")
    print(f"   ‚Ä¢ FastMCP: {'‚úÖ' if FASTMCP_AVAILABLE else '‚ùå'}")
    
    if OLLAMA_AVAILABLE:
        print(f"üß† Configuration Agent:")
        print(f"   ‚Ä¢ Mod√®le: {workflow_orchestrator.ollama_model}")
        print(f"   ‚Ä¢ Contextes: {len(workflow_orchestrator.agent_contexts)} disponibles")
        print(f"   ‚Ä¢ Strat√©gies: Sequential, Parallel, Hybrid")
    
    print(f"üéØ Capacit√©s:")
    print(f"   ‚Ä¢ Planification intelligente de workflows")
    print(f"   ‚Ä¢ Ex√©cution adaptative (sequential/parallel/hybrid)")
    print(f"   ‚Ä¢ R√©cup√©ration automatique d'erreurs")
    print(f"   ‚Ä¢ Optimisation en temps r√©el")
    print(f"   ‚Ä¢ Analyse de performance et insights")
    print(f"   ‚Ä¢ Apprentissage continu des patterns")
    
    # Test rapide de l'agent
    if OLLAMA_AVAILABLE:
        print(f"\nüß™ Test de l'agent...")
        try:
            import asyncio
            
            async def test_agent():
                test_request = {
                    "type": "batch_generation",
                    "signs": ["aries", "taurus"],
                    "include_video": True,
                    "optimization_goals": ["speed", "quality"]
                }
                
                plan = await workflow_orchestrator.create_intelligent_plan(test_request)
                print(f"‚úÖ Agent op√©rationnel - Plan cr√©√©: {plan.execution_strategy}")
                return True
            
            # Ex√©cuter le test
            test_result = asyncio.run(test_agent())
            if test_result:
                print("üöÄ Agent pr√™t pour orchestration intelligente!")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Test agent √©chou√©: {e}")
            print("üí° V√©rifiez qu'Ollama est d√©marr√© et que le mod√®le est disponible")
    
    # D√©marrage du serveur MCP
    if FASTMCP_AVAILABLE:
        print(f"\nüåê D√©marrage serveur MCP...")
        print(f"üîß Outils disponibles:")
        print(f"   ‚Ä¢ create_intelligent_workflow")
        print(f"   ‚Ä¢ optimize_existing_workflow") 
        print(f"   ‚Ä¢ get_orchestrator_status")
        print(f"   ‚Ä¢ analyze_workflow_performance")
        
        print(f"\nüí° Utilisation depuis app.py:")
        print(f"   from workflow_orchestrator_mcp import workflow_orchestrator")
        print(f"   result = await workflow_orchestrator.create_intelligent_plan(request)")
        
        mcp.run()
    else:
        print("‚ùå FastMCP non disponible - Serveur MCP non d√©marr√©")
        print("üí° pip install fastmcp pour activer le serveur")
        print("üìÑ L'orchestrateur peut √™tre utilis√© en mode module direct")